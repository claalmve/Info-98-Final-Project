{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info 98 Final Project\n",
    "#### By Calvin Chen, Jusheen Kim, Jamila Hussein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datascience import *\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "from matplotlib import patches\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "sqrt = np.sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_in = pd.read_csv(\"processed.hungarian.data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data extraction and cleaning\n",
    "cleaning, cleaned = open('haberman.data.txt').read().split(\"\\n\"), Table(['Age', 'Year Op', 'Num of Pos Aux Nodes', 'Survival Status'])\n",
    "cleaning = cleaning[0:len(cleaning)-1]\n",
    "for i in range(len(cleaning)):\n",
    "    cleaned = cleaned.with_row([int(j) for j in cleaning[i].split(',')])\n",
    "random_cleaned = cleaned.sample(with_replacement=False)\n",
    "\n",
    "train_set = random_cleaned.take(np.arange(0, int(random_cleaned.num_rows * 0.8)))\n",
    "train_status, train_set = train_set.column('Survival Status'), train_set.drop('Survival Status')\n",
    "\n",
    "test_set = random_cleaned.take(np.arange(int(random_cleaned.num_rows * 0.8), random_cleaned.num_rows))\n",
    "test_status, test_set = test_set.column('Survival Status'), test_set.drop('Survival Status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE (Used for kNN to determine nearest neighbors and used for LinReg to determine accuracy of the regression)\n",
    "def RMSE(array1, array2):\n",
    "    return sqrt(sum((array1 - array2) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the kNN Model\n",
    "def distances(test_row, train_set):\n",
    "    distances = make_array()\n",
    "    for row in train_set.rows:\n",
    "        distances = np.append(distances, RMSE(make_array(row)[0], test_row))\n",
    "    return distances\n",
    "\n",
    "def classify(test_row, train_set, k):\n",
    "    classes_and_distances = Table().with_columns('Test Status', train_status, 'Distances', distances(test_row, train_set)).sort('Distances')\n",
    "    return classes_and_distances.take(np.arange(k)).group('Test Status').sort('count', descending=True).column(0).item(0)\n",
    "\n",
    "def accuracy(test_set, train_set, k):\n",
    "    classes = make_array()\n",
    "    for row in test_set.rows:\n",
    "        classes = np.append(classes, classify(make_array(row)[0], train_set, k))\n",
    "    comparison = Table().with_columns('Predicted Status', classes, 'Actual Status', test_status)\n",
    "    return np.count_nonzero(comparison.column(0) == comparison.column(1)) / len(comparison.column(0)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "best_k = Table(['k', 'Accuracy'])\n",
    "for i in range(1, 20): # Only using k-values from 1 to 20\n",
    "    best_k = best_k.with_row([i, accuracy(test_set, train_set, i)])\n",
    "best_k.where('Accuracy', are.equal_to(max(best_k.column(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the linear regression\n",
    "def standard_units(xyz):\n",
    "    return (xyz - np.mean(xyz))/np.std(xyz)\n",
    "\n",
    "def correlation(t, label_x, label_y):\n",
    "    return np.mean(standard_units(t.column(label_x))*standard_units(t.column(label_y)))\n",
    "\n",
    "def slope(t, label_x, label_y):\n",
    "    r = correlation(t, label_x, label_y)\n",
    "    return r*np.std(t.column(label_y))/np.std(t.column(label_x))\n",
    "\n",
    "def intercept(t, label_x, label_y):\n",
    "    return np.mean(t.column(label_y)) - slope(t, label_x, label_y)*np.mean(t.column(label_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression on Training Set w/ Residuals\n",
    "intercept, slope = intercept(train_set, 'Num of Pos Aux Nodes', 'Age'), slope(train_set, 'Num of Pos Aux Nodes', 'Age')\n",
    "\n",
    "lin_train = train_set\n",
    "guesses = lin_train.with_column('Guesses', slope * train_set.column('Num of Pos Aux Nodes') + intercept)\n",
    "residuals = guesses.with_column('Residuals', guesses.column('Guesses') - guesses.column('Age'))\n",
    "\n",
    "accuracy = np.count_nonzero(np.round(residuals.column('Residuals')) == 0) / len(residuals.column('Residuals'))\n",
    "print(\"Training set accuracy: %s\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "test_ages, test_nodes = test_set.column('Age'), test_set.column('Num of Pos Aux Nodes')\n",
    "accuracy = np.count_nonzero(np.round(test_ages - (slope * test_nodes + intercept)) == 0) / len(test_nodes)\n",
    "print(\"Training set accuracy: %s\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failed Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing = train_set.with_column('Guess', slope * train_set.column(2) + intercept)\n",
    "# residuals = testing.with_column('Residuals', testing.column('Guess') - testing.column(1))\n",
    "# residuals.scatter('Num of Pos Aux Nodes', 'Residuals')\n",
    "# residuals.scatter('Residuals', 'Age')\n",
    "# residuals.scatter('Num of Pos Aux Nodes', 'Age', fit_line=True)\n",
    "\n",
    "# residuals.column('Residuals')\n",
    "\n",
    "# for i in cleaned.columns:\n",
    "#     for j cleaned.column(j):\n",
    "#         try:\n",
    "#             j = int(j)\n",
    "#         except:\n",
    "#             # skip\n",
    "\n",
    "# intercept = intercept(train_set, 'Num of Pos Aux Nodes', 'Age')\n",
    "# slope = slope(train_set, 'Num of Pos Aux Nodes', 'Age')\n",
    "\n",
    "# testing = train_set.with_column('Guess', slope * train_set.column(2) + intercept)\n",
    "# residuals = testing.with_column('Residuals', testing.column('Guess') - testing.column(1))\n",
    "\n",
    "\n",
    "\n",
    "# rows = open('haberman.data.txt').read().split(\"\\n\")\n",
    "# test = Table(['1','2','3','4'])\n",
    "# test.with_row([int(i) for i in rows[0].split(',')])\n",
    "\n",
    "# cleaned = cleaned.with_row([int(j) for j in cleaning[0].split(',')])\n",
    "# type(cleaned.column(0).item(0))\n",
    "# cleaning[0].split(',')\n",
    "\n",
    "# adult_data = open('adult.data.txt').read()\n",
    "\n",
    "# cleaner = []\n",
    "# for i in range(len(cleaning)):\n",
    "#     cleaner.append(cleaning[i].split(\", \"))\n",
    "# cleaned = Table(['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15'])\n",
    "# for i in range(10000):\n",
    "#     cleaned = cleaned.with_row(cleaner[i])\n",
    "# cleaned\n",
    "\n",
    "# adult_data = open('adult.data.txt').read()\n",
    "# cleaned_data = adult_data.split('\\n')\n",
    "# splitted = make_array()\n",
    "# for i in range(1000):\n",
    "#     chunk = cleaned_data[i].split(', ')\n",
    "# #     splitted += chunk\n",
    "#     print(chunk)\n",
    "#     splitted = np.append(splitted, list(chunk))\n",
    "#     print(splitted)\n",
    "# # cleaned_data[0].split(', ')\n",
    "# # splitted = cleaned_data[0:15]\n",
    "# # splitted1 = cleaned_data[15:30]\n",
    "# # splitted, splitted1\n",
    "\n",
    "\n",
    "\n",
    "# splitted = []\n",
    "\n",
    "# def split(data):\n",
    "#     if len(data) <= 15:\n",
    "#         return data\n",
    "#     else:\n",
    "#         one_row = data[0:15]\n",
    "# #       np.append(splitted, one_row)\n",
    "#         splitted.append(one_row)\n",
    "#         return split(data[15:])\n",
    "\n",
    "# split(short_data)\n",
    "# # test = Table(['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15'])\n",
    "# # # for row in splitted:\n",
    "# # #     print(row)\n",
    "# # test.with_row(make_array(np.arange(16)))\n",
    "# # test\n",
    "# table = Table(['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15'])\n",
    "# for i in range(len(splitted)):\n",
    "#     table.with_row(splitted[i])\n",
    "# splitted\n",
    "\n",
    "# # tiles = Table(['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15'])\n",
    "# # tiles.with_row(splitted[0])\n",
    "# # tiles.with_rows(make_array(make_array('c', 2, 3), make_array('d', 4, 2)))\n",
    "\n",
    "# # test.with_rows([np.arange(17)])\n",
    "\n",
    "# # adult_data = pd.read_csv('adult.data.txt')\n",
    "# # adult_data = open('adult.data.txt')\n",
    "# # adult_data = open('adult.data.txt').read()\n",
    "# # cleaned_data = adult_data.split(', ')\n",
    "# # Table.read_table(cleaned_data)\n",
    "# # require(adult_data)\n",
    "# # setDT(adult_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
